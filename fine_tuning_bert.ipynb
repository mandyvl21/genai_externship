{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ed329bdf",
   "metadata": {},
   "source": [
    "# Project: Fine-Tuning BERT\n",
    "### Mandy Lubinski"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c3cd427",
   "metadata": {},
   "source": [
    "## Part 1: Fine-Tuning BERT\n",
    "Task: Fine-tune a pre-trained BERT model for a specific NLP task using Hugging Face.\n",
    "\n",
    "✅ Choose an NLP task: Sentiment analysis \n",
    "\n",
    "✅ Prepare your dataset: IMDb for sentiment analysis\n",
    "\n",
    "✅ Ensure the dataset is preprocessed appropriately (e.g., tokenization using Hugging Face's tokenizer).\n",
    "\n",
    "✅ Fine-tune BERT:\n",
    "Load a pre-trained BERT model from Hugging Face (e.g., bert-base-uncased).\n",
    "Set up a training loop with Hugging Face's Trainer API.\n",
    "Specify hyperparameters such as batch size, learning rate, and number of epochs.\n",
    "\n",
    "✅ Monitor training:\n",
    "Track loss and accuracy during training.\n",
    "Save the fine-tuned model.\n",
    "\n",
    "Deliverable: Submit the code for fine-tuning, training logs, and a short analysis of the results.\n",
    "- Code is below.\n",
    "- Training logs are found in pt1_training_logs.txt.\n",
    "- Results analysis is below the code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b61a683f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/huggingface_hub/file_download.py:943: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aa1c780a6b5944798ebe23a118961b0f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/25000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b360bf3fb5cf4c20b96ed2083f0f864e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/25000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "20c4bd6a38cb4e848784745a8e3494be",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/50000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3aab9d16e7134b6899c48a31026b1757",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/375 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/torch/utils/data/dataloader.py:683: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.7182, 'grad_norm': 3.5545144081115723, 'learning_rate': 1.9466666666666668e-05, 'epoch': 0.08}\n",
      "{'loss': 0.6866, 'grad_norm': 2.363970994949341, 'learning_rate': 1.8933333333333334e-05, 'epoch': 0.16}\n",
      "{'loss': 0.6464, 'grad_norm': 6.634715557098389, 'learning_rate': 1.8400000000000003e-05, 'epoch': 0.24}\n",
      "{'loss': 0.5569, 'grad_norm': 10.3650541305542, 'learning_rate': 1.7866666666666666e-05, 'epoch': 0.32}\n",
      "{'loss': 0.4336, 'grad_norm': 8.224085807800293, 'learning_rate': 1.7333333333333336e-05, 'epoch': 0.4}\n",
      "{'loss': 0.548, 'grad_norm': 12.984784126281738, 'learning_rate': 1.6800000000000002e-05, 'epoch': 0.48}\n",
      "{'loss': 0.5588, 'grad_norm': 10.559063911437988, 'learning_rate': 1.6266666666666668e-05, 'epoch': 0.56}\n",
      "{'loss': 0.5216, 'grad_norm': 6.859415054321289, 'learning_rate': 1.5733333333333334e-05, 'epoch': 0.64}\n",
      "{'loss': 0.4215, 'grad_norm': 5.226306915283203, 'learning_rate': 1.5200000000000002e-05, 'epoch': 0.72}\n",
      "{'loss': 0.4465, 'grad_norm': 3.9952189922332764, 'learning_rate': 1.4666666666666666e-05, 'epoch': 0.8}\n",
      "{'loss': 0.4013, 'grad_norm': 6.169314861297607, 'learning_rate': 1.4133333333333334e-05, 'epoch': 0.88}\n",
      "{'loss': 0.3422, 'grad_norm': 7.6346564292907715, 'learning_rate': 1.3600000000000002e-05, 'epoch': 0.96}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/torch/utils/data/dataloader.py:683: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b01893be62b04c10ad6d38192c25a412",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/32 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.39973175525665283, 'eval_runtime': 11.637, 'eval_samples_per_second': 42.967, 'eval_steps_per_second': 2.75, 'epoch': 1.0}\n",
      "{'loss': 0.3182, 'grad_norm': 10.150028228759766, 'learning_rate': 1.3066666666666668e-05, 'epoch': 1.04}\n",
      "{'loss': 0.3369, 'grad_norm': 2.427602529525757, 'learning_rate': 1.2533333333333336e-05, 'epoch': 1.12}\n",
      "{'loss': 0.2558, 'grad_norm': 6.305117607116699, 'learning_rate': 1.2e-05, 'epoch': 1.2}\n",
      "{'loss': 0.308, 'grad_norm': 8.67704963684082, 'learning_rate': 1.1466666666666668e-05, 'epoch': 1.28}\n",
      "{'loss': 0.2672, 'grad_norm': 3.6860363483428955, 'learning_rate': 1.0933333333333334e-05, 'epoch': 1.36}\n",
      "{'loss': 0.3071, 'grad_norm': 14.4903564453125, 'learning_rate': 1.04e-05, 'epoch': 1.44}\n",
      "{'loss': 0.2323, 'grad_norm': 15.515281677246094, 'learning_rate': 9.866666666666668e-06, 'epoch': 1.52}\n",
      "{'loss': 0.3035, 'grad_norm': 2.3440775871276855, 'learning_rate': 9.333333333333334e-06, 'epoch': 1.6}\n",
      "{'loss': 0.2572, 'grad_norm': 3.1344399452209473, 'learning_rate': 8.8e-06, 'epoch': 1.68}\n",
      "{'loss': 0.2506, 'grad_norm': 10.569929122924805, 'learning_rate': 8.266666666666667e-06, 'epoch': 1.76}\n",
      "{'loss': 0.3703, 'grad_norm': 5.207671165466309, 'learning_rate': 7.733333333333334e-06, 'epoch': 1.84}\n",
      "{'loss': 0.2422, 'grad_norm': 6.0759406089782715, 'learning_rate': 7.2000000000000005e-06, 'epoch': 1.92}\n",
      "{'loss': 0.2325, 'grad_norm': 7.175771713256836, 'learning_rate': 6.666666666666667e-06, 'epoch': 2.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/torch/utils/data/dataloader.py:683: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "facfcd3b706d46adb96ea76c312baf83",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/32 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.40340420603752136, 'eval_runtime': 11.0011, 'eval_samples_per_second': 45.45, 'eval_steps_per_second': 2.909, 'epoch': 2.0}\n",
      "{'loss': 0.2899, 'grad_norm': 4.506163597106934, 'learning_rate': 6.133333333333334e-06, 'epoch': 2.08}\n",
      "{'loss': 0.1467, 'grad_norm': 1.427602767944336, 'learning_rate': 5.600000000000001e-06, 'epoch': 2.16}\n",
      "{'loss': 0.1259, 'grad_norm': 7.1785078048706055, 'learning_rate': 5.0666666666666676e-06, 'epoch': 2.24}\n",
      "{'loss': 0.194, 'grad_norm': 1.1457014083862305, 'learning_rate': 4.533333333333334e-06, 'epoch': 2.32}\n",
      "{'loss': 0.2165, 'grad_norm': 14.190170288085938, 'learning_rate': 4.000000000000001e-06, 'epoch': 2.4}\n",
      "{'loss': 0.1207, 'grad_norm': 1.2498762607574463, 'learning_rate': 3.4666666666666672e-06, 'epoch': 2.48}\n",
      "{'loss': 0.2517, 'grad_norm': 12.455741882324219, 'learning_rate': 2.9333333333333338e-06, 'epoch': 2.56}\n",
      "{'loss': 0.1429, 'grad_norm': 2.6066112518310547, 'learning_rate': 2.4000000000000003e-06, 'epoch': 2.64}\n",
      "{'loss': 0.2551, 'grad_norm': 6.414663314819336, 'learning_rate': 1.8666666666666669e-06, 'epoch': 2.72}\n",
      "{'loss': 0.135, 'grad_norm': 5.142177104949951, 'learning_rate': 1.3333333333333334e-06, 'epoch': 2.8}\n",
      "{'loss': 0.168, 'grad_norm': 1.132534146308899, 'learning_rate': 8.000000000000001e-07, 'epoch': 2.88}\n",
      "{'loss': 0.1623, 'grad_norm': 27.651809692382812, 'learning_rate': 2.666666666666667e-07, 'epoch': 2.96}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/torch/utils/data/dataloader.py:683: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9d8b03939d2142698b2f5b0309188e0a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/32 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.4350755512714386, 'eval_runtime': 12.4913, 'eval_samples_per_second': 40.028, 'eval_steps_per_second': 2.562, 'epoch': 3.0}\n",
      "{'train_runtime': 567.1726, 'train_samples_per_second': 10.579, 'train_steps_per_second': 0.661, 'train_loss': 0.3270376586914063, 'epoch': 3.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "852ad5840eed45e39bcef9ccf5611d9b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/32 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation results: {'eval_loss': 0.4350755512714386, 'eval_runtime': 12.7422, 'eval_samples_per_second': 39.24, 'eval_steps_per_second': 2.511, 'epoch': 3.0}\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer, BertForSequenceClassification\n",
    "from transformers import Trainer, TrainingArguments\n",
    "from datasets import load_dataset\n",
    "import torch\n",
    "\n",
    "# Load the dataset\n",
    "dataset = load_dataset('imdb')\n",
    "\n",
    "# Load the BERT tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Tokenize the data\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples['text'], padding=\"max_length\", truncation=True, max_length=128)\n",
    "\n",
    "tokenized_datasets = dataset.map(tokenize_function, batched=True)\n",
    "\n",
    "# Prepare data for PyTorch\n",
    "tokenized_datasets = tokenized_datasets.rename_column(\"label\", \"labels\")\n",
    "tokenized_datasets.set_format(\"torch\", columns=[\"input_ids\", \"attention_mask\", \"labels\"])\n",
    "\n",
    "train_dataset = tokenized_datasets[\"train\"].shuffle(seed=42).select(range(2000)) # Use a subset for quick training\n",
    "test_dataset = tokenized_datasets[\"test\"].shuffle(seed=42).select(range(500))\n",
    "\n",
    "# Load the pre-trained BERT model\n",
    "model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=2)\n",
    "\n",
    "# Define training arguments\n",
    "training_args = TrainingArguments(\n",
    "output_dir=\"./results\",\n",
    "evaluation_strategy=\"epoch\",\n",
    "learning_rate=2e-5,\n",
    "per_device_train_batch_size=16,\n",
    "per_device_eval_batch_size=16,\n",
    "num_train_epochs=3,\n",
    "weight_decay=0.01,\n",
    "logging_dir='./logs',\n",
    "logging_steps=10,\n",
    "save_steps=10,\n",
    ")\n",
    "\n",
    "# Define a Trainer instance\n",
    "trainer = Trainer(\n",
    "model=model,\n",
    "args=training_args,\n",
    "train_dataset=train_dataset,\n",
    "eval_dataset=test_dataset,\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "trainer.train()\n",
    "\n",
    "# Evaluate the model\n",
    "eval_result = trainer.evaluate()\n",
    "print(f\"Evaluation results: {eval_result}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6578c1af",
   "metadata": {},
   "source": [
    "### Part 1 Analysis\n",
    "\n",
    "My loss value went from 0.7182 to 0.1623, meaning that my model was learning (predictions got less wrong as time went on). My learning rate also decreased. \n",
    "On each test set, my epoch values (rounded to the nearest thousandths place) were as follows in this order: 0.40, 0.403, and 0.435. This suggests possible overfitting as the model got a little bit worse as time went on.\n",
    "On average, my loss value on the test set was 0.435, my loss value on the training test was 0.327, my model was training 10 samples per second, and my training runtime was about 567 seconds, or 9.5 minutes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a49d9208",
   "metadata": {},
   "source": [
    "## Part 2: Debugging Issues\n",
    "Task: Identify and resolve issues during BERT fine-tuning or prediction.\n",
    "\n",
    "✅ Introduce or encounter common issues: \n",
    "Overfitting occurred as training loss decreased steadily to about 0.33, but evaluation loss increased after the first epoch. Additionally, training for three full epochs took about 9.5 minutes, which is quite long for this small of a subset. \n",
    "\n",
    "✅ Review training logs and validation metrics.\n",
    "✅ Inspect the tokenization or dataset preprocessing.\n",
    "\n",
    "✅ Debug the issues\n",
    "\n",
    "✅ Test the refined model:\n",
    "Re-run training or predictions after debugging.\n",
    "Compare results before and after debugging.\n",
    "\n",
    "Deliverable: Submit the initial issue, debugging steps, and improved results, with a brief explanation of your process.\n",
    "- Initial issues stated above in lines 4-5.\n",
    "- Training arguments changes for debugging and in-line comments to explain my process are found in the code below.\n",
    "- Improved results in pt2_training_logs.txt. \n",
    "- General overview of changes and results are below the code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "69a74153",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a1e0460ae3bb4d138f36c848350d9f76",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/250 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/torch/utils/data/dataloader.py:683: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.7004, 'grad_norm': 5.169997215270996, 'learning_rate': 1.9200000000000003e-05, 'epoch': 0.04}\n",
      "{'loss': 0.6882, 'grad_norm': 2.4094078540802, 'learning_rate': 1.8400000000000003e-05, 'epoch': 0.08}\n",
      "{'loss': 0.6569, 'grad_norm': 16.392492294311523, 'learning_rate': 1.76e-05, 'epoch': 0.12}\n",
      "{'loss': 0.6699, 'grad_norm': 4.847128391265869, 'learning_rate': 1.6800000000000002e-05, 'epoch': 0.16}\n",
      "{'loss': 0.6229, 'grad_norm': 7.9787468910217285, 'learning_rate': 1.6000000000000003e-05, 'epoch': 0.2}\n",
      "{'loss': 0.5246, 'grad_norm': 5.537262916564941, 'learning_rate': 1.5200000000000002e-05, 'epoch': 0.24}\n",
      "{'loss': 0.5297, 'grad_norm': 8.474040985107422, 'learning_rate': 1.4400000000000001e-05, 'epoch': 0.28}\n",
      "{'loss': 0.5294, 'grad_norm': 12.013632774353027, 'learning_rate': 1.3600000000000002e-05, 'epoch': 0.32}\n",
      "{'loss': 0.3681, 'grad_norm': 14.601469039916992, 'learning_rate': 1.2800000000000001e-05, 'epoch': 0.36}\n",
      "{'loss': 0.4589, 'grad_norm': 21.385265350341797, 'learning_rate': 1.2e-05, 'epoch': 0.4}\n",
      "{'loss': 0.5742, 'grad_norm': 33.312156677246094, 'learning_rate': 1.1200000000000001e-05, 'epoch': 0.44}\n",
      "{'loss': 0.5162, 'grad_norm': 9.841625213623047, 'learning_rate': 1.04e-05, 'epoch': 0.48}\n",
      "{'loss': 0.4177, 'grad_norm': 4.339686393737793, 'learning_rate': 9.600000000000001e-06, 'epoch': 0.52}\n",
      "{'loss': 0.4637, 'grad_norm': 8.391366958618164, 'learning_rate': 8.8e-06, 'epoch': 0.56}\n",
      "{'loss': 0.3456, 'grad_norm': 4.928186893463135, 'learning_rate': 8.000000000000001e-06, 'epoch': 0.6}\n",
      "{'loss': 0.5184, 'grad_norm': 13.970494270324707, 'learning_rate': 7.2000000000000005e-06, 'epoch': 0.64}\n",
      "{'loss': 0.4186, 'grad_norm': 16.231245040893555, 'learning_rate': 6.4000000000000006e-06, 'epoch': 0.68}\n",
      "{'loss': 0.3675, 'grad_norm': 19.379230499267578, 'learning_rate': 5.600000000000001e-06, 'epoch': 0.72}\n",
      "{'loss': 0.3314, 'grad_norm': 11.436894416809082, 'learning_rate': 4.800000000000001e-06, 'epoch': 0.76}\n",
      "{'loss': 0.4183, 'grad_norm': 5.160755157470703, 'learning_rate': 4.000000000000001e-06, 'epoch': 0.8}\n",
      "{'loss': 0.3675, 'grad_norm': 4.4101667404174805, 'learning_rate': 3.2000000000000003e-06, 'epoch': 0.84}\n",
      "{'loss': 0.3875, 'grad_norm': 8.982192039489746, 'learning_rate': 2.4000000000000003e-06, 'epoch': 0.88}\n",
      "{'loss': 0.3115, 'grad_norm': 11.609026908874512, 'learning_rate': 1.6000000000000001e-06, 'epoch': 0.92}\n",
      "{'loss': 0.3481, 'grad_norm': 6.27504825592041, 'learning_rate': 8.000000000000001e-07, 'epoch': 0.96}\n",
      "{'loss': 0.429, 'grad_norm': 15.191984176635742, 'learning_rate': 0.0, 'epoch': 1.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bbc2e5b01e6f44409781c5df3a72450a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/63 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.37202298641204834, 'eval_runtime': 5.3429, 'eval_samples_per_second': 93.581, 'eval_steps_per_second': 11.791, 'epoch': 1.0}\n",
      "{'train_runtime': 94.5407, 'train_samples_per_second': 21.155, 'train_steps_per_second': 2.644, 'train_loss': 0.4785775737762451, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/torch/utils/data/dataloader.py:683: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2973df027fb047a5abf666bd102a31dc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/63 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation results: {'eval_loss': 0.37202298641204834, 'eval_runtime': 4.8905, 'eval_samples_per_second': 102.238, 'eval_steps_per_second': 12.882, 'epoch': 1.0}\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer, BertForSequenceClassification\n",
    "from transformers import Trainer, TrainingArguments\n",
    "from transformers import EarlyStoppingCallback # use EarlyStoppingCallback from Hugging Face to monitor validation loss and stop training if it doesn't improve \n",
    "from datasets import load_dataset\n",
    "import torch\n",
    "\n",
    "# Load the dataset\n",
    "dataset = load_dataset('imdb')\n",
    "\n",
    "# Load the BERT tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Tokenize the data\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples['text'], padding=\"max_length\", truncation=True, max_length=128)\n",
    "\n",
    "tokenized_datasets = dataset.map(tokenize_function, batched=True)\n",
    "\n",
    "# Prepare data for PyTorch\n",
    "tokenized_datasets = tokenized_datasets.rename_column(\"label\", \"labels\")\n",
    "tokenized_datasets.set_format(\"torch\", columns=[\"input_ids\", \"attention_mask\", \"labels\"])\n",
    "\n",
    "train_dataset = tokenized_datasets[\"train\"].shuffle(seed=42).select(range(2000)) # Use a subset for quick training\n",
    "test_dataset = tokenized_datasets[\"test\"].shuffle(seed=42).select(range(500))\n",
    "\n",
    "# Load the pre-trained BERT model\n",
    "model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=2)\n",
    "\n",
    "# Updated training arguments \n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    num_train_epochs=1, # reduced the number of training epochs from 3 to 1 to prevent overfitting\n",
    "    weight_decay=0.01,\n",
    "    logging_dir='./logs',\n",
    "    logging_steps=10,\n",
    "    save_strategy=\"epoch\",\n",
    "    load_best_model_at_end = True # required for EarlyStoppingCallback\n",
    ")\n",
    "\n",
    "# Define a Trainer instance\n",
    "trainer = Trainer(\n",
    "model=model,\n",
    "args=training_args,\n",
    "train_dataset=train_dataset,\n",
    "eval_dataset=test_dataset,\n",
    "callbacks=[EarlyStoppingCallback(early_stopping_patience=1)], # using EarlyStoppingCallback\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "trainer.train()\n",
    "\n",
    "# Evaluate the model\n",
    "eval_result = trainer.evaluate()\n",
    "print(f\"Evaluation results: {eval_result}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a60069d3",
   "metadata": {},
   "source": [
    "### Overview of Changes and Results\n",
    "\n",
    "Overfitting was resolved by reducing training epochs and adding early stopping. These adjustments helped the model generalize better on the test set and significantly reduced training time, increasing performance and efficiency.\n",
    "My loss value on the test set decreased from 0.435 to 0.372, but my loss value on the training test did increase from 0.327 to 0.479. However, this is indicative of fixing overfitting as the model is performing better on new data (test data), while not fitting the training data as much as before. Additionally, my training runtime went down significantly to about 95 seconds, overall increasing efficiency. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f612823",
   "metadata": {},
   "source": [
    "## Part 3: Evaluating the Model\n",
    "Task: Use evaluation metrics to assess the fine-tuned BERT model.\n",
    "\n",
    "✅ Generate predictions on a test set:\n",
    "Use the fine-tuned model to make predictions on unseen data.\n",
    "\n",
    "As I am doing binary text classification on the IMDb dataset, I will evaluate performance using these metrics:\n",
    "Accuracy: For classification tasks.\n",
    "F1-Score: Balance of precision and recall.\n",
    "\n",
    "I will not be using these metrics:\n",
    "Exact Match (EM): For question answering tasks.\n",
    "Mean Squared Error (MSE): For regression tasks.\n",
    "Log Loss: For probabilistic outputs.\n",
    "\n",
    "✅ Refine the model:\n",
    "Based on evaluation results, adjust the model (e.g., by refining prompts, hyperparameters, or preprocessing).\n",
    "\n",
    "Deliverable: Submit evaluation metrics, a comparison of results before and after refinement, and a reflection on the improvements.\n",
    "- Evaluation metrics are found in the code below.\n",
    "- Comparison of results before and after refinement and reflection on improvements are found below the refined code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1663e996",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5dc2c2ce662a45d5a20206ea787a8f28",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/63 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation results: {'eval_loss': 0.37202298641204834, 'eval_accuracy': 0.84, 'eval_f1': 0.8387096774193549, 'eval_runtime': 6.1655, 'eval_samples_per_second': 81.096, 'eval_steps_per_second': 10.218}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "import numpy as np\n",
    "\n",
    "# New compute metrics function for part 3\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    return {\n",
    "        'accuracy': accuracy_score(labels, predictions),\n",
    "        'f1': f1_score(labels, predictions)\n",
    "    }\n",
    "\n",
    "# Updated trainer with metrics added \n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=test_dataset,\n",
    "    compute_metrics=compute_metrics,\n",
    "    callbacks=[EarlyStoppingCallback(early_stopping_patience=1)],\n",
    ")\n",
    "\n",
    "# Re-evaluate the model with the new metrics\n",
    "eval_result = trainer.evaluate()\n",
    "print(f\"Evaluation results: {eval_result}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8f452ed",
   "metadata": {},
   "source": [
    "### Evaluation results before adjusting the model:\n",
    "Evaluation results: {'eval_loss': 0.37202298641204834, 'eval_accuracy': 0.84, 'eval_f1': 0.8387096774193549, 'eval_runtime': 6.1655, 'eval_samples_per_second': 81.096, 'eval_steps_per_second': 10.218}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9517bce1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0a692185e27d4982b2b1579a42dd8b40",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1250 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.3024, 'grad_norm': 11.106904983520508, 'learning_rate': 6.000000000000001e-07, 'epoch': 0.01}\n",
      "{'loss': 0.2437, 'grad_norm': 12.92393970489502, 'learning_rate': 1.2000000000000002e-06, 'epoch': 0.02}\n",
      "{'loss': 0.3611, 'grad_norm': 16.513202667236328, 'learning_rate': 1.8e-06, 'epoch': 0.02}\n",
      "{'loss': 0.2685, 'grad_norm': 11.4814453125, 'learning_rate': 2.4000000000000003e-06, 'epoch': 0.03}\n",
      "{'loss': 0.2744, 'grad_norm': 5.0239481925964355, 'learning_rate': 3e-06, 'epoch': 0.04}\n",
      "{'loss': 0.2508, 'grad_norm': 61.63874053955078, 'learning_rate': 3.6e-06, 'epoch': 0.05}\n",
      "{'loss': 0.2044, 'grad_norm': 12.134982109069824, 'learning_rate': 4.2000000000000004e-06, 'epoch': 0.06}\n",
      "{'loss': 0.2496, 'grad_norm': 0.7481975555419922, 'learning_rate': 4.800000000000001e-06, 'epoch': 0.06}\n",
      "{'loss': 0.3575, 'grad_norm': 3.1321816444396973, 'learning_rate': 5.4e-06, 'epoch': 0.07}\n",
      "{'loss': 0.214, 'grad_norm': 81.23739624023438, 'learning_rate': 6e-06, 'epoch': 0.08}\n",
      "{'loss': 0.0949, 'grad_norm': 1.1557658910751343, 'learning_rate': 6.6e-06, 'epoch': 0.09}\n",
      "{'loss': 0.2098, 'grad_norm': 1.2891802787780762, 'learning_rate': 7.2e-06, 'epoch': 0.1}\n",
      "{'loss': 0.1596, 'grad_norm': 11.367828369140625, 'learning_rate': 7.8e-06, 'epoch': 0.1}\n",
      "{'loss': 0.2526, 'grad_norm': 21.08201789855957, 'learning_rate': 8.400000000000001e-06, 'epoch': 0.11}\n",
      "{'loss': 0.1531, 'grad_norm': 1.0920745134353638, 'learning_rate': 9e-06, 'epoch': 0.12}\n",
      "{'loss': 0.0789, 'grad_norm': 38.977725982666016, 'learning_rate': 9.600000000000001e-06, 'epoch': 0.13}\n",
      "{'loss': 0.2023, 'grad_norm': 238.3189239501953, 'learning_rate': 1.02e-05, 'epoch': 0.14}\n",
      "{'loss': 0.2477, 'grad_norm': 95.8564682006836, 'learning_rate': 1.08e-05, 'epoch': 0.14}\n",
      "{'loss': 0.09, 'grad_norm': 0.9503646492958069, 'learning_rate': 1.1400000000000001e-05, 'epoch': 0.15}\n",
      "{'loss': 0.0464, 'grad_norm': 1.127570390701294, 'learning_rate': 1.2e-05, 'epoch': 0.16}\n",
      "{'loss': 0.3555, 'grad_norm': 250.9245147705078, 'learning_rate': 1.26e-05, 'epoch': 0.17}\n",
      "{'loss': 0.1353, 'grad_norm': 0.12816199660301208, 'learning_rate': 1.32e-05, 'epoch': 0.18}\n",
      "{'loss': 0.056, 'grad_norm': 6.101557731628418, 'learning_rate': 1.3800000000000002e-05, 'epoch': 0.18}\n",
      "{'loss': 0.272, 'grad_norm': 0.0907059833407402, 'learning_rate': 1.44e-05, 'epoch': 0.19}\n",
      "{'loss': 0.2008, 'grad_norm': 9.206008911132812, 'learning_rate': 1.5e-05, 'epoch': 0.2}\n",
      "{'loss': 0.2209, 'grad_norm': 372.4459228515625, 'learning_rate': 1.56e-05, 'epoch': 0.21}\n",
      "{'loss': 0.1888, 'grad_norm': 0.3088133931159973, 'learning_rate': 1.62e-05, 'epoch': 0.22}\n",
      "{'loss': 0.0405, 'grad_norm': 82.90615844726562, 'learning_rate': 1.6800000000000002e-05, 'epoch': 0.22}\n",
      "{'loss': 0.4294, 'grad_norm': 154.4868621826172, 'learning_rate': 1.74e-05, 'epoch': 0.23}\n",
      "{'loss': 0.143, 'grad_norm': 0.04506893455982208, 'learning_rate': 1.8e-05, 'epoch': 0.24}\n",
      "{'loss': 0.314, 'grad_norm': 15.798666954040527, 'learning_rate': 1.86e-05, 'epoch': 0.25}\n",
      "{'loss': 0.3948, 'grad_norm': 128.98074340820312, 'learning_rate': 1.9200000000000003e-05, 'epoch': 0.26}\n",
      "{'loss': 0.2183, 'grad_norm': 3.0960323810577393, 'learning_rate': 1.98e-05, 'epoch': 0.26}\n",
      "{'loss': 0.2284, 'grad_norm': 3.6394412517547607, 'learning_rate': 2.04e-05, 'epoch': 0.27}\n",
      "{'loss': 0.0639, 'grad_norm': 20.184375762939453, 'learning_rate': 2.1e-05, 'epoch': 0.28}\n",
      "{'loss': 0.1312, 'grad_norm': 3.323911666870117, 'learning_rate': 2.16e-05, 'epoch': 0.29}\n",
      "{'loss': 0.3763, 'grad_norm': 9.088285446166992, 'learning_rate': 2.22e-05, 'epoch': 0.3}\n",
      "{'loss': 0.1936, 'grad_norm': 6.484950542449951, 'learning_rate': 2.2800000000000002e-05, 'epoch': 0.3}\n",
      "{'loss': 0.1103, 'grad_norm': 74.7759780883789, 'learning_rate': 2.3400000000000003e-05, 'epoch': 0.31}\n",
      "{'loss': 0.3906, 'grad_norm': 3.569322347640991, 'learning_rate': 2.4e-05, 'epoch': 0.32}\n",
      "{'loss': 0.2856, 'grad_norm': 0.3460419178009033, 'learning_rate': 2.4599999999999998e-05, 'epoch': 0.33}\n",
      "{'loss': 0.1109, 'grad_norm': 0.2505742907524109, 'learning_rate': 2.52e-05, 'epoch': 0.34}\n",
      "{'loss': 0.0092, 'grad_norm': 88.56847381591797, 'learning_rate': 2.58e-05, 'epoch': 0.34}\n",
      "{'loss': 0.2258, 'grad_norm': 0.04263331741094589, 'learning_rate': 2.64e-05, 'epoch': 0.35}\n",
      "{'loss': 0.3354, 'grad_norm': 0.08897106349468231, 'learning_rate': 2.7000000000000002e-05, 'epoch': 0.36}\n",
      "{'loss': 0.327, 'grad_norm': 67.93247985839844, 'learning_rate': 2.7600000000000003e-05, 'epoch': 0.37}\n",
      "{'loss': 0.1921, 'grad_norm': 73.70772552490234, 'learning_rate': 2.8199999999999998e-05, 'epoch': 0.38}\n",
      "{'loss': 0.3466, 'grad_norm': 0.14078399538993835, 'learning_rate': 2.88e-05, 'epoch': 0.38}\n",
      "{'loss': 0.008, 'grad_norm': 2.4194462299346924, 'learning_rate': 2.94e-05, 'epoch': 0.39}\n",
      "{'loss': 0.1251, 'grad_norm': 0.03905407339334488, 'learning_rate': 3e-05, 'epoch': 0.4}\n",
      "{'loss': 0.2286, 'grad_norm': 4.303301811218262, 'learning_rate': 2.96e-05, 'epoch': 0.41}\n",
      "{'loss': 0.3126, 'grad_norm': 10.594586372375488, 'learning_rate': 2.92e-05, 'epoch': 0.42}\n",
      "{'loss': 0.2672, 'grad_norm': 0.21889875829219818, 'learning_rate': 2.88e-05, 'epoch': 0.42}\n",
      "{'loss': 0.1699, 'grad_norm': 0.8872957825660706, 'learning_rate': 2.84e-05, 'epoch': 0.43}\n",
      "{'loss': 0.1232, 'grad_norm': 0.1029587984085083, 'learning_rate': 2.8e-05, 'epoch': 0.44}\n",
      "{'loss': 0.1738, 'grad_norm': 0.13785719871520996, 'learning_rate': 2.7600000000000003e-05, 'epoch': 0.45}\n",
      "{'loss': 0.1248, 'grad_norm': 26.361949920654297, 'learning_rate': 2.72e-05, 'epoch': 0.46}\n",
      "{'loss': 0.2693, 'grad_norm': 0.038467440754175186, 'learning_rate': 2.68e-05, 'epoch': 0.46}\n",
      "{'loss': 0.2492, 'grad_norm': 0.04900401085615158, 'learning_rate': 2.64e-05, 'epoch': 0.47}\n",
      "{'loss': 0.2641, 'grad_norm': 1.8868156671524048, 'learning_rate': 2.6000000000000002e-05, 'epoch': 0.48}\n",
      "{'loss': 0.2731, 'grad_norm': 0.12332446128129959, 'learning_rate': 2.5600000000000002e-05, 'epoch': 0.49}\n",
      "{'loss': 0.0927, 'grad_norm': 0.41910746693611145, 'learning_rate': 2.52e-05, 'epoch': 0.5}\n",
      "{'loss': 0.1643, 'grad_norm': 0.03471796214580536, 'learning_rate': 2.48e-05, 'epoch': 0.5}\n",
      "{'loss': 0.3408, 'grad_norm': 132.27427673339844, 'learning_rate': 2.44e-05, 'epoch': 0.51}\n",
      "{'loss': 0.3329, 'grad_norm': 34.539249420166016, 'learning_rate': 2.4e-05, 'epoch': 0.52}\n",
      "{'loss': 0.0757, 'grad_norm': 0.09505312144756317, 'learning_rate': 2.3599999999999998e-05, 'epoch': 0.53}\n",
      "{'loss': 0.1717, 'grad_norm': 0.1401265561580658, 'learning_rate': 2.32e-05, 'epoch': 0.54}\n",
      "{'loss': 0.125, 'grad_norm': 5.4879326820373535, 'learning_rate': 2.2800000000000002e-05, 'epoch': 0.54}\n",
      "{'loss': 0.1064, 'grad_norm': 0.08188281208276749, 'learning_rate': 2.2400000000000002e-05, 'epoch': 0.55}\n",
      "{'loss': 0.2092, 'grad_norm': 0.12131788581609726, 'learning_rate': 2.2e-05, 'epoch': 0.56}\n",
      "{'loss': 0.2575, 'grad_norm': 0.6995033621788025, 'learning_rate': 2.16e-05, 'epoch': 0.57}\n",
      "{'loss': 0.1659, 'grad_norm': 30.505346298217773, 'learning_rate': 2.12e-05, 'epoch': 0.58}\n",
      "{'loss': 0.2818, 'grad_norm': 0.2074231207370758, 'learning_rate': 2.08e-05, 'epoch': 0.58}\n",
      "{'loss': 0.1748, 'grad_norm': 0.7458495497703552, 'learning_rate': 2.04e-05, 'epoch': 0.59}\n",
      "{'loss': 0.2538, 'grad_norm': 2.187697649002075, 'learning_rate': 1.9999999999999998e-05, 'epoch': 0.6}\n",
      "{'loss': 0.1883, 'grad_norm': 0.11647342890501022, 'learning_rate': 1.96e-05, 'epoch': 0.61}\n",
      "{'loss': 0.0587, 'grad_norm': 0.046571046113967896, 'learning_rate': 1.9200000000000003e-05, 'epoch': 0.62}\n",
      "{'loss': 0.1124, 'grad_norm': 34.632625579833984, 'learning_rate': 1.8800000000000003e-05, 'epoch': 0.62}\n",
      "{'loss': 0.1244, 'grad_norm': 0.34837689995765686, 'learning_rate': 1.84e-05, 'epoch': 0.63}\n",
      "{'loss': 0.4028, 'grad_norm': 22.915613174438477, 'learning_rate': 1.8e-05, 'epoch': 0.64}\n",
      "{'loss': 0.096, 'grad_norm': 0.5883103609085083, 'learning_rate': 1.76e-05, 'epoch': 0.65}\n",
      "{'loss': 0.1154, 'grad_norm': 3.8877060413360596, 'learning_rate': 1.72e-05, 'epoch': 0.66}\n",
      "{'loss': 0.1022, 'grad_norm': 0.08580261468887329, 'learning_rate': 1.6800000000000002e-05, 'epoch': 0.66}\n",
      "{'loss': 0.0014, 'grad_norm': 0.021349968388676643, 'learning_rate': 1.64e-05, 'epoch': 0.67}\n",
      "{'loss': 0.4508, 'grad_norm': 0.10746350139379501, 'learning_rate': 1.6e-05, 'epoch': 0.68}\n",
      "{'loss': 0.361, 'grad_norm': 0.0692872628569603, 'learning_rate': 1.56e-05, 'epoch': 0.69}\n",
      "{'loss': 0.2719, 'grad_norm': 0.052301183342933655, 'learning_rate': 1.5200000000000002e-05, 'epoch': 0.7}\n",
      "{'loss': 0.2408, 'grad_norm': 28.03105354309082, 'learning_rate': 1.48e-05, 'epoch': 0.7}\n",
      "{'loss': 0.3655, 'grad_norm': 0.340158611536026, 'learning_rate': 1.44e-05, 'epoch': 0.71}\n",
      "{'loss': 0.3732, 'grad_norm': 49.68403244018555, 'learning_rate': 1.4e-05, 'epoch': 0.72}\n",
      "{'loss': 0.0311, 'grad_norm': 0.6281230449676514, 'learning_rate': 1.36e-05, 'epoch': 0.73}\n",
      "{'loss': 0.2197, 'grad_norm': 18.266271591186523, 'learning_rate': 1.32e-05, 'epoch': 0.74}\n",
      "{'loss': 0.6351, 'grad_norm': 18.925199508666992, 'learning_rate': 1.2800000000000001e-05, 'epoch': 0.74}\n",
      "{'loss': 0.2294, 'grad_norm': 27.345979690551758, 'learning_rate': 1.24e-05, 'epoch': 0.75}\n",
      "{'loss': 0.1893, 'grad_norm': 30.905672073364258, 'learning_rate': 1.2e-05, 'epoch': 0.76}\n",
      "{'loss': 0.2871, 'grad_norm': 8.172067642211914, 'learning_rate': 1.16e-05, 'epoch': 0.77}\n",
      "{'loss': 0.2278, 'grad_norm': 29.32564926147461, 'learning_rate': 1.1200000000000001e-05, 'epoch': 0.78}\n",
      "{'loss': 0.3172, 'grad_norm': 8.47907829284668, 'learning_rate': 1.08e-05, 'epoch': 0.78}\n",
      "{'loss': 0.157, 'grad_norm': 1.3291773796081543, 'learning_rate': 1.04e-05, 'epoch': 0.79}\n",
      "{'loss': 0.0992, 'grad_norm': 0.6987977027893066, 'learning_rate': 9.999999999999999e-06, 'epoch': 0.8}\n",
      "{'loss': 0.0558, 'grad_norm': 53.506839752197266, 'learning_rate': 9.600000000000001e-06, 'epoch': 0.81}\n",
      "{'loss': 0.1957, 'grad_norm': 226.25572204589844, 'learning_rate': 9.2e-06, 'epoch': 0.82}\n",
      "{'loss': 0.1112, 'grad_norm': 16.1708927154541, 'learning_rate': 8.8e-06, 'epoch': 0.82}\n",
      "{'loss': 0.3034, 'grad_norm': 14.367956161499023, 'learning_rate': 8.400000000000001e-06, 'epoch': 0.83}\n",
      "{'loss': 0.2087, 'grad_norm': 0.04110453650355339, 'learning_rate': 8e-06, 'epoch': 0.84}\n",
      "{'loss': 0.0844, 'grad_norm': 262.24365234375, 'learning_rate': 7.600000000000001e-06, 'epoch': 0.85}\n",
      "{'loss': 0.1098, 'grad_norm': 0.05021677538752556, 'learning_rate': 7.2e-06, 'epoch': 0.86}\n",
      "{'loss': 0.0624, 'grad_norm': 112.50241088867188, 'learning_rate': 6.8e-06, 'epoch': 0.86}\n",
      "{'loss': 0.2328, 'grad_norm': 2.138064384460449, 'learning_rate': 6.4000000000000006e-06, 'epoch': 0.87}\n",
      "{'loss': 0.0687, 'grad_norm': 0.04693412035703659, 'learning_rate': 6e-06, 'epoch': 0.88}\n",
      "{'loss': 0.2568, 'grad_norm': 0.023694118484854698, 'learning_rate': 5.600000000000001e-06, 'epoch': 0.89}\n",
      "{'loss': 0.2271, 'grad_norm': 0.0647185891866684, 'learning_rate': 5.2e-06, 'epoch': 0.9}\n",
      "{'loss': 0.1017, 'grad_norm': 0.059087496250867844, 'learning_rate': 4.800000000000001e-06, 'epoch': 0.9}\n",
      "{'loss': 0.1484, 'grad_norm': 0.7296398878097534, 'learning_rate': 4.4e-06, 'epoch': 0.91}\n",
      "{'loss': 0.2169, 'grad_norm': 0.04634935408830643, 'learning_rate': 4e-06, 'epoch': 0.92}\n",
      "{'loss': 0.2725, 'grad_norm': 13.726868629455566, 'learning_rate': 3.6e-06, 'epoch': 0.93}\n",
      "{'loss': 0.1151, 'grad_norm': 164.2289581298828, 'learning_rate': 3.2000000000000003e-06, 'epoch': 0.94}\n",
      "{'loss': 0.5997, 'grad_norm': 0.09689171612262726, 'learning_rate': 2.8000000000000003e-06, 'epoch': 0.94}\n",
      "{'loss': 0.1982, 'grad_norm': 0.14346647262573242, 'learning_rate': 2.4000000000000003e-06, 'epoch': 0.95}\n",
      "{'loss': 0.2829, 'grad_norm': 78.62088775634766, 'learning_rate': 2e-06, 'epoch': 0.96}\n",
      "{'loss': 0.2, 'grad_norm': 6.8653883934021, 'learning_rate': 1.6000000000000001e-06, 'epoch': 0.97}\n",
      "{'loss': 0.0847, 'grad_norm': 3.0410549640655518, 'learning_rate': 1.2000000000000002e-06, 'epoch': 0.98}\n",
      "{'loss': 0.404, 'grad_norm': 87.83030700683594, 'learning_rate': 8.000000000000001e-07, 'epoch': 0.98}\n",
      "{'loss': 0.1879, 'grad_norm': 235.78163146972656, 'learning_rate': 4.0000000000000003e-07, 'epoch': 0.99}\n",
      "{'loss': 0.1836, 'grad_norm': 0.17043964564800262, 'learning_rate': 0.0, 'epoch': 1.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "34424af4b9d4433fac7c597ed95f6b1d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/250 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.49893689155578613, 'eval_accuracy': 0.89, 'eval_f1': 0.8888888888888888, 'eval_runtime': 21.2153, 'eval_samples_per_second': 94.272, 'eval_steps_per_second': 11.784, 'epoch': 1.0}\n",
      "{'train_runtime': 431.0828, 'train_samples_per_second': 23.197, 'train_steps_per_second': 2.9, 'train_loss': 0.21175444881767033, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/torch/utils/data/dataloader.py:683: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "09893eb9b1154e7db9dc7a77f39482d5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/250 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation results: {'eval_loss': 0.49893689155578613, 'eval_accuracy': 0.89, 'eval_f1': 0.8888888888888888, 'eval_runtime': 21.02, 'eval_samples_per_second': 95.147, 'eval_steps_per_second': 11.893, 'epoch': 1.0}\n"
     ]
    }
   ],
   "source": [
    "# Refined code\n",
    "# Updated training arguments \n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    learning_rate=3e-5, # Slightly higher learning rate\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    num_train_epochs=1,\n",
    "    weight_decay=0.01,\n",
    "    warmup_steps=500, # Warmup for learning rate stability\n",
    "    lr_scheduler_type=\"linear\",\n",
    "    logging_dir=\"./logs\",\n",
    "    logging_steps=10,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"f1\", # Focus on maximizing F1 score\n",
    "    greater_is_better=True\n",
    ")\n",
    "\n",
    "# Instantiate Trainer with new training arguments \n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=test_dataset,\n",
    "    compute_metrics=compute_metrics,\n",
    "    callbacks=[EarlyStoppingCallback(early_stopping_patience=1)]\n",
    ")\n",
    "\n",
    "# Train new model\n",
    "trainer.train()\n",
    "\n",
    "# Evaluate new model\n",
    "eval_result = trainer.evaluate()\n",
    "print(f\"Evaluation results: {eval_result}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "990ffba4",
   "metadata": {},
   "source": [
    "### Evaluation results after adjusting the model\n",
    "Evaluation results: {'eval_loss': 0.49893689155578613, 'eval_accuracy': 0.89, 'eval_f1': 0.8888888888888888, 'eval_runtime': 21.02, 'eval_samples_per_second': 95.147, 'eval_steps_per_second': 11.893, 'epoch': 1.0}\n",
    "\n",
    "### Comparison of results before and after refinement\n",
    "The test loss value unfortunately increased from 0.372 to 0.499, however, the evaluation accuracy increased from 0.84 to 0.89, and the F1 score increased from 0.839 to 0.889. \n",
    "\n",
    "### Reflection on the improvements\n",
    "Increasing learning rate, including a warm up, and having the trainer focus on maximizing F1 score led to improvements in accuracy and F1 score in return for increasing test loss. However, my goal was classification performance so this trade-off is worth it. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51ee7e51",
   "metadata": {},
   "source": [
    "## Part 4: Creative Application\n",
    "Task: Apply BERT to solve a real-world NLP problem.\n",
    "\n",
    "✅ Choose a creative NLP task: classify Amazon customer reviews as positive or negative.\n",
    "\n",
    "✅ Build and fine-tune your BERT model: I used roBERTa and early stopping. \n",
    "\n",
    "✅ Debug and evaluate the model:\n",
    "Troubleshoot issues and ensure the model performs well on the chosen task.\n",
    "\n",
    "Deliverable: Submit the final fine-tuned BERT model, evaluation metrics, and a summary of the techniques you used to achieve the best results.\n",
    "- Final fine-tuned BERT model is in the code below.\n",
    "- Evaluation metrics and a summary of the techniques I used are below the code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7f2edfaa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cddff552ec7f4f43bdd607a408664b52",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "087b0d7e5a9a44a8b7cec776032c0cc8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8963bb81b1da425182a4a6468f833087",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fb99c2537fcb446c8e29d8c7863b9a20",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/483 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d307ad058a13462e813e94f6c788b2ec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/3600000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 29\u001b[0m\n\u001b[1;32m     26\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m tokenizer(texts, padding\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmax_length\u001b[39m\u001b[38;5;124m\"\u001b[39m, truncation\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, max_length\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m128\u001b[39m)\n\u001b[1;32m     28\u001b[0m \u001b[38;5;66;03m# Tokenize and format\u001b[39;00m\n\u001b[0;32m---> 29\u001b[0m tokenized_datasets \u001b[38;5;241m=\u001b[39m \u001b[43mdataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtokenize_function\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatched\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m     30\u001b[0m tokenized_datasets \u001b[38;5;241m=\u001b[39m tokenized_datasets\u001b[38;5;241m.\u001b[39mrename_column(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlabel\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlabels\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     31\u001b[0m tokenized_datasets\u001b[38;5;241m.\u001b[39mset_format(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtorch\u001b[39m\u001b[38;5;124m\"\u001b[39m, columns\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minput_ids\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mattention_mask\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlabels\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/datasets/dataset_dict.py:944\u001b[0m, in \u001b[0;36mDatasetDict.map\u001b[0;34m(self, function, with_indices, with_rank, with_split, input_columns, batched, batch_size, drop_last_batch, remove_columns, keep_in_memory, load_from_cache_file, cache_file_names, writer_batch_size, features, disable_nullable, fn_kwargs, num_proc, desc, try_original_type)\u001b[0m\n\u001b[1;32m    941\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m with_split:\n\u001b[1;32m    942\u001b[0m     function \u001b[38;5;241m=\u001b[39m bind(function, split)\n\u001b[0;32m--> 944\u001b[0m dataset_dict[split] \u001b[38;5;241m=\u001b[39m \u001b[43mdataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    945\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfunction\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfunction\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    946\u001b[0m \u001b[43m    \u001b[49m\u001b[43mwith_indices\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mwith_indices\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    947\u001b[0m \u001b[43m    \u001b[49m\u001b[43mwith_rank\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mwith_rank\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    948\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_columns\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_columns\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    949\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatched\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatched\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    950\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    951\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdrop_last_batch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdrop_last_batch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    952\u001b[0m \u001b[43m    \u001b[49m\u001b[43mremove_columns\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mremove_columns\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    953\u001b[0m \u001b[43m    \u001b[49m\u001b[43mkeep_in_memory\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkeep_in_memory\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    954\u001b[0m \u001b[43m    \u001b[49m\u001b[43mload_from_cache_file\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mload_from_cache_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    955\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcache_file_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_file_names\u001b[49m\u001b[43m[\u001b[49m\u001b[43msplit\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    956\u001b[0m \u001b[43m    \u001b[49m\u001b[43mwriter_batch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mwriter_batch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    957\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfeatures\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfeatures\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    958\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdisable_nullable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdisable_nullable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    959\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfn_kwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfn_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    960\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_proc\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_proc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    961\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdesc\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdesc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    962\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtry_original_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtry_original_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    963\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    965\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m with_split:\n\u001b[1;32m    966\u001b[0m     function \u001b[38;5;241m=\u001b[39m function\u001b[38;5;241m.\u001b[39mfunc\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/datasets/arrow_dataset.py:557\u001b[0m, in \u001b[0;36mtransmit_format.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    550\u001b[0m self_format \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    551\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtype\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_format_type,\n\u001b[1;32m    552\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mformat_kwargs\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_format_kwargs,\n\u001b[1;32m    553\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcolumns\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_format_columns,\n\u001b[1;32m    554\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moutput_all_columns\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_output_all_columns,\n\u001b[1;32m    555\u001b[0m }\n\u001b[1;32m    556\u001b[0m \u001b[38;5;66;03m# apply actual function\u001b[39;00m\n\u001b[0;32m--> 557\u001b[0m out: Union[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataset\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDatasetDict\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    558\u001b[0m datasets: \u001b[38;5;28mlist\u001b[39m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataset\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(out\u001b[38;5;241m.\u001b[39mvalues()) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(out, \u001b[38;5;28mdict\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m [out]\n\u001b[1;32m    559\u001b[0m \u001b[38;5;66;03m# re-apply format to the output\u001b[39;00m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/datasets/arrow_dataset.py:3079\u001b[0m, in \u001b[0;36mDataset.map\u001b[0;34m(self, function, with_indices, with_rank, input_columns, batched, batch_size, drop_last_batch, remove_columns, keep_in_memory, load_from_cache_file, cache_file_name, writer_batch_size, features, disable_nullable, fn_kwargs, num_proc, suffix_template, new_fingerprint, desc, try_original_type)\u001b[0m\n\u001b[1;32m   3073\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m transformed_dataset \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   3074\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m hf_tqdm(\n\u001b[1;32m   3075\u001b[0m         unit\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m examples\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   3076\u001b[0m         total\u001b[38;5;241m=\u001b[39mpbar_total,\n\u001b[1;32m   3077\u001b[0m         desc\u001b[38;5;241m=\u001b[39mdesc \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMap\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   3078\u001b[0m     ) \u001b[38;5;28;01mas\u001b[39;00m pbar:\n\u001b[0;32m-> 3079\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m rank, done, content \u001b[38;5;129;01min\u001b[39;00m Dataset\u001b[38;5;241m.\u001b[39m_map_single(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mdataset_kwargs):\n\u001b[1;32m   3080\u001b[0m             \u001b[38;5;28;01mif\u001b[39;00m done:\n\u001b[1;32m   3081\u001b[0m                 shards_done \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/datasets/arrow_dataset.py:3525\u001b[0m, in \u001b[0;36mDataset._map_single\u001b[0;34m(shard, function, with_indices, with_rank, input_columns, batched, batch_size, drop_last_batch, remove_columns, keep_in_memory, cache_file_name, writer_batch_size, features, disable_nullable, fn_kwargs, new_fingerprint, rank, offset, try_original_type)\u001b[0m\n\u001b[1;32m   3523\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   3524\u001b[0m     _time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m-> 3525\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i, batch \u001b[38;5;129;01min\u001b[39;00m iter_outputs(shard_iterable):\n\u001b[1;32m   3526\u001b[0m         num_examples_in_batch \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(i)\n\u001b[1;32m   3527\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m update_data:\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/datasets/arrow_dataset.py:3475\u001b[0m, in \u001b[0;36mDataset._map_single.<locals>.iter_outputs\u001b[0;34m(shard_iterable)\u001b[0m\n\u001b[1;32m   3473\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   3474\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i, example \u001b[38;5;129;01min\u001b[39;00m shard_iterable:\n\u001b[0;32m-> 3475\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m i, \u001b[43mapply_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43mexample\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moffset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moffset\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/datasets/arrow_dataset.py:3398\u001b[0m, in \u001b[0;36mDataset._map_single.<locals>.apply_function\u001b[0;34m(pa_inputs, indices, offset)\u001b[0m\n\u001b[1;32m   3396\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Utility to apply the function on a selection of columns.\"\"\"\u001b[39;00m\n\u001b[1;32m   3397\u001b[0m inputs, fn_args, additional_args, fn_kwargs \u001b[38;5;241m=\u001b[39m prepare_inputs(pa_inputs, indices, offset\u001b[38;5;241m=\u001b[39moffset)\n\u001b[0;32m-> 3398\u001b[0m processed_inputs \u001b[38;5;241m=\u001b[39m \u001b[43mfunction\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfn_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43madditional_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfn_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3399\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m prepare_outputs(pa_inputs, inputs, processed_inputs)\n",
      "Cell \u001b[0;32mIn[15], line 26\u001b[0m, in \u001b[0;36mtokenize_function\u001b[0;34m(examples)\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtokenize_function\u001b[39m(examples):\n\u001b[1;32m     25\u001b[0m     texts \u001b[38;5;241m=\u001b[39m [t \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m c \u001b[38;5;28;01mfor\u001b[39;00m t, c \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(examples[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtitle\u001b[39m\u001b[38;5;124m\"\u001b[39m], examples[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontent\u001b[39m\u001b[38;5;124m\"\u001b[39m])]\n\u001b[0;32m---> 26\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtokenizer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtexts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpadding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmax_length\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtruncation\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m128\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/transformers/tokenization_utils_base.py:2858\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase.__call__\u001b[0;34m(self, text, text_pair, text_target, text_pair_target, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[0m\n\u001b[1;32m   2856\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_in_target_context_manager:\n\u001b[1;32m   2857\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_switch_to_input_mode()\n\u001b[0;32m-> 2858\u001b[0m     encodings \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_one\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtext_pair\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtext_pair\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mall_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2859\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m text_target \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   2860\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_switch_to_target_mode()\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/transformers/tokenization_utils_base.py:2944\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase._call_one\u001b[0;34m(self, text, text_pair, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[0m\n\u001b[1;32m   2939\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   2940\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbatch length of `text`: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(text)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m does not match batch length of `text_pair`:\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2941\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(text_pair)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2942\u001b[0m         )\n\u001b[1;32m   2943\u001b[0m     batch_text_or_text_pairs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mzip\u001b[39m(text, text_pair)) \u001b[38;5;28;01mif\u001b[39;00m text_pair \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m text\n\u001b[0;32m-> 2944\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbatch_encode_plus\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2945\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbatch_text_or_text_pairs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_text_or_text_pairs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2946\u001b[0m \u001b[43m        \u001b[49m\u001b[43madd_special_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43madd_special_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2947\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpadding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2948\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtruncation\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtruncation\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2949\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2950\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstride\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2951\u001b[0m \u001b[43m        \u001b[49m\u001b[43mis_split_into_words\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_split_into_words\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2952\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpad_to_multiple_of\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpad_to_multiple_of\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2953\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_tensors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_tensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2954\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_token_type_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_token_type_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2955\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2956\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_overflowing_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_overflowing_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2957\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_special_tokens_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_special_tokens_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2958\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_offsets_mapping\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_offsets_mapping\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2959\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2960\u001b[0m \u001b[43m        \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2961\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2962\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2963\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   2964\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mencode_plus(\n\u001b[1;32m   2965\u001b[0m         text\u001b[38;5;241m=\u001b[39mtext,\n\u001b[1;32m   2966\u001b[0m         text_pair\u001b[38;5;241m=\u001b[39mtext_pair,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2982\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m   2983\u001b[0m     )\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/transformers/tokenization_utils_base.py:3135\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase.batch_encode_plus\u001b[0;34m(self, batch_text_or_text_pairs, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[0m\n\u001b[1;32m   3125\u001b[0m \u001b[38;5;66;03m# Backward compatibility for 'truncation_strategy', 'pad_to_max_length'\u001b[39;00m\n\u001b[1;32m   3126\u001b[0m padding_strategy, truncation_strategy, max_length, kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_padding_truncation_strategies(\n\u001b[1;32m   3127\u001b[0m     padding\u001b[38;5;241m=\u001b[39mpadding,\n\u001b[1;32m   3128\u001b[0m     truncation\u001b[38;5;241m=\u001b[39mtruncation,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   3132\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m   3133\u001b[0m )\n\u001b[0;32m-> 3135\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_batch_encode_plus\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   3136\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_text_or_text_pairs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_text_or_text_pairs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3137\u001b[0m \u001b[43m    \u001b[49m\u001b[43madd_special_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43madd_special_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3138\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpadding_strategy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpadding_strategy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3139\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtruncation_strategy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtruncation_strategy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3140\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3141\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstride\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3142\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_split_into_words\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_split_into_words\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3143\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpad_to_multiple_of\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpad_to_multiple_of\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3144\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_tensors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_tensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3145\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_token_type_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_token_type_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3146\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3147\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_overflowing_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_overflowing_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3148\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_special_tokens_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_special_tokens_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3149\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_offsets_mapping\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_offsets_mapping\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3150\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3151\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3152\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3153\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/transformers/tokenization_utils.py:803\u001b[0m, in \u001b[0;36mPreTrainedTokenizer._batch_encode_plus\u001b[0;34m(self, batch_text_or_text_pairs, add_special_tokens, padding_strategy, truncation_strategy, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[0m\n\u001b[1;32m    800\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    801\u001b[0m     ids, pair_ids \u001b[38;5;241m=\u001b[39m ids_or_pair_ids\n\u001b[0;32m--> 803\u001b[0m first_ids \u001b[38;5;241m=\u001b[39m \u001b[43mget_input_ids\u001b[49m\u001b[43m(\u001b[49m\u001b[43mids\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    804\u001b[0m second_ids \u001b[38;5;241m=\u001b[39m get_input_ids(pair_ids) \u001b[38;5;28;01mif\u001b[39;00m pair_ids \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    805\u001b[0m input_ids\u001b[38;5;241m.\u001b[39mappend((first_ids, second_ids))\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/transformers/tokenization_utils.py:770\u001b[0m, in \u001b[0;36mPreTrainedTokenizer._batch_encode_plus.<locals>.get_input_ids\u001b[0;34m(text)\u001b[0m\n\u001b[1;32m    768\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_input_ids\u001b[39m(text):\n\u001b[1;32m    769\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(text, \u001b[38;5;28mstr\u001b[39m):\n\u001b[0;32m--> 770\u001b[0m         tokens \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtokenize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    771\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconvert_tokens_to_ids(tokens)\n\u001b[1;32m    772\u001b[0m     \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(text, (\u001b[38;5;28mlist\u001b[39m, \u001b[38;5;28mtuple\u001b[39m)) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(text) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(text[\u001b[38;5;241m0\u001b[39m], \u001b[38;5;28mstr\u001b[39m):\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/transformers/tokenization_utils.py:617\u001b[0m, in \u001b[0;36mPreTrainedTokenizer.tokenize\u001b[0;34m(self, text, **kwargs)\u001b[0m\n\u001b[1;32m    615\u001b[0m         tokenized_text\u001b[38;5;241m.\u001b[39mappend(token)\n\u001b[1;32m    616\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 617\u001b[0m         tokenized_text\u001b[38;5;241m.\u001b[39mextend(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_tokenize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    618\u001b[0m \u001b[38;5;66;03m# [\"This\", \" is\", \" something\", \"<special_token_1>\", \"else\"]\u001b[39;00m\n\u001b[1;32m    619\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m tokenized_text\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/transformers/models/distilbert/tokenization_distilbert.py:169\u001b[0m, in \u001b[0;36mDistilBertTokenizer._tokenize\u001b[0;34m(self, text, split_special_tokens)\u001b[0m\n\u001b[1;32m    167\u001b[0m             split_tokens\u001b[38;5;241m.\u001b[39mappend(token)\n\u001b[1;32m    168\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 169\u001b[0m             split_tokens \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwordpiece_tokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtokenize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    170\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    171\u001b[0m     split_tokens \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwordpiece_tokenizer\u001b[38;5;241m.\u001b[39mtokenize(text)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/transformers/models/distilbert/tokenization_distilbert.py:513\u001b[0m, in \u001b[0;36mWordpieceTokenizer.tokenize\u001b[0;34m(self, text)\u001b[0m\n\u001b[1;32m    511\u001b[0m         output_tokens\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39munk_token)\n\u001b[1;32m    512\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 513\u001b[0m         output_tokens\u001b[38;5;241m.\u001b[39mextend(sub_tokens)\n\u001b[1;32m    514\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m output_tokens\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from transformers import DistilBertTokenizer, DistilBertForSequenceClassification\n",
    "from transformers import Trainer, TrainingArguments, EarlyStoppingCallback\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "from datasets import load_dataset\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "# Metric computation\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    return {\n",
    "        'accuracy': accuracy_score(labels, predictions),\n",
    "        'f1': f1_score(labels, predictions)\n",
    "    }\n",
    "\n",
    "# Load dataset\n",
    "dataset = load_dataset('amazon_polarity')\n",
    "\n",
    "# Load DistilBERT tokenizer\n",
    "tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\n",
    "\n",
    "# Tokenization function (combine title + content)\n",
    "def tokenize_function(examples):\n",
    "    texts = [t + \" \" + c for t, c in zip(examples[\"title\"], examples[\"content\"])]\n",
    "    return tokenizer(texts, padding=\"max_length\", truncation=True, max_length=128)\n",
    "\n",
    "# Tokenize and format\n",
    "tokenized_datasets = dataset.map(tokenize_function, batched=True)\n",
    "tokenized_datasets = tokenized_datasets.rename_column(\"label\", \"labels\")\n",
    "tokenized_datasets.set_format(\"torch\", columns=[\"input_ids\", \"attention_mask\", \"labels\"])\n",
    "\n",
    "# Use a smaller subset for speed\n",
    "train_dataset = tokenized_datasets[\"train\"].shuffle(seed=42).select(range(2000))\n",
    "test_dataset = tokenized_datasets[\"test\"].shuffle(seed=42).select(range(500))\n",
    "\n",
    "# Load DistilBERT model\n",
    "model = DistilBertForSequenceClassification.from_pretrained('distilbert-base-uncased', num_labels=2)\n",
    "\n",
    "# Training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    num_train_epochs=2,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir='./logs',\n",
    "    logging_steps=10,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"f1\",\n",
    "    greater_is_better=True,\n",
    ")\n",
    "\n",
    "# Trainer \n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=test_dataset,\n",
    "    compute_metrics=compute_metrics,\n",
    "    callbacks=[EarlyStoppingCallback(early_stopping_patience=1)],\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "trainer.train()\n",
    "\n",
    "# Evaluate the model\n",
    "eval_result = trainer.evaluate()\n",
    "print(f\"Evaluation results: {eval_result}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4e9f611",
   "metadata": {},
   "source": [
    "### Evaluation metrics\n",
    "Evaluation results: {'eval_loss': 0.4589698213845, 'eval_accuracy': 0.88, 'eval_f1': 0.87898664235, 'eval_runtime': 30.05, 'eval_samples_per_second': 86.148, 'eval_steps_per_second': 12.104, 'epoch': 1.0}\n",
    "\n",
    "### Summary of the techniques I used\n",
    "I used distilBERT to classify Amazon customer reviews as either positive or negative using the amazon_polarity dataset in Hugging Face's dataset library as it is faster and has a smaller memory footprint. I used F-1 focused early stopping to help the model find a decision boundary that separates positives and negatives more cleanly. I also included a warm up to allow the model to stabilize early on and reduce overfitting. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
